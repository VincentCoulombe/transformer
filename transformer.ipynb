{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'architecture\n",
    "Probablement la raison pourquoi vous regardez ce notebook, j'ai commenté ma démarche. Vous aller\n",
    "retrouver la version anglaise des noms de plusieurs des concepts vues dans le blogue.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size: int,\n",
    "                 num_heads: int,\n",
    "                 dropout: float,\n",
    "                 forward_expansion: int=4\n",
    "                 ):\n",
    "        \"\"\"Un bloc de transformation composé d'une couche de self-attention,\n",
    "            de deux couches de normalisation, d'une couche de dropout et d'un réseau feedforward.\n",
    "\n",
    "        Args:\n",
    "            embedding_size (int): La taille des embeddings\n",
    "            num_heads (int): Le nombre de tête de l'attention multi-tête\n",
    "            dropout (float): Le % de dropout\n",
    "            forward_expansion (int): La taille de la couche cachée du réseau feedforward par\n",
    "                                    rapport à la taille des embeddings (4 dans le paper)\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # La couche de self-attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_size, num_heads=num_heads)\n",
    "        \n",
    "        # On ajoute des couches de normalisation et de dropout (plus de performances)\n",
    "        self.norm1 = nn.LayerNorm(embedding_size)\n",
    "        self.norm2 = nn.LayerNorm(embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Le réseau feedforward\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_size, forward_expansion * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embedding_size, embedding_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, value, key, query, mask=None):\n",
    "        # On passe les embeddings dans la couche de self-attention\n",
    "        attn_output, attn_scores = self.multihead_attn(query, key, value)\n",
    "        # Note: attn_scores permet de visualiser l'attention (si désiré)\n",
    "        \n",
    "        # On normalise et on dropout la sortie de la couche de self-attention avec la connexion résiduelle\n",
    "        normalized_attn_out = self.dropout(self.norm1(attn_output + query)) \n",
    "        \n",
    "        # On passe x dans le réseau feedforward\n",
    "        forward_output = self.feed_forward(normalized_attn_out)\n",
    "        \n",
    "        # On normalise et on dropout la sortie du réseau feedforward avec la connexion résiduelle\n",
    "        return self.dropout(self.norm2(forward_output + normalized_attn_out)), attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 seq_length: int,\n",
    "                 embedding_size: int,\n",
    "                 num_classes: int, \n",
    "                 num_layers: int, \n",
    "                 num_heads: int, \n",
    "                 dropout: float,\n",
    "                 forward_expansion: int=4, \n",
    "                 ):\n",
    "        \"\"\"Un classifieur transformer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): La taille du vocabulaire, \n",
    "                            c'est-à-dire le nombre de tokens différents (sort du cadre de ce blogue)\n",
    "            seq_length (int): La taille des séquences d'entrée\n",
    "            embedding_size (int): La taille de nos embeddings\n",
    "            num_classes (int): Le nombre de classes à prédire\n",
    "            num_layers (int): Le nombre de blocs de transformation\n",
    "            num_heads (int): Le nombre de têtes d'attention dans chaque bloc de transformation\n",
    "            dropout (float): Le % de dropout à utiliser\n",
    "            forward_expansion (int): La taille de la couche cachée du réseau feedforward par\n",
    "                            rapport à la taille des embeddings (4 dans le paper)\n",
    "        \"\"\"\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        # Les embeddings des tokens\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Les embeddings de position\n",
    "        self.position_embedding = nn.Embedding(seq_length, embedding_size)\n",
    "        \n",
    "        # Il y a num_layers blocs de transformation\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embedding_size, num_heads, dropout, forward_expansion) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Dropout pour éviter le sur-apprentissage (plus de performances)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # On ajoute une couche linéaire pour la classification (sortir 2 classes)\n",
    "        self.fc = nn.Linear(embedding_size, num_classes)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length = x.shape\n",
    "        batch_positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(x.device)\n",
    "        positioned_x = self.position_embedding(batch_positions) + self.embedding(x)\n",
    "        out = self.dropout(positioned_x)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            out = block(value=out, key=out, query=out, mask=mask)\n",
    "        out = out.mean(dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le jeux de données\n",
    "\n",
    "https://paperswithcode.com/dataset/imdb-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "r = requests.get(url)\n",
    "with open(\"aclImdb_v1.tar.gz\", \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "    \n",
    "with tarfile.open(\"aclImdb_v1.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, phase, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.phase = phase\n",
    "        self.comments = []\n",
    "        self.attention_masks = []\n",
    "        self.labels = []\n",
    "        for label in [\"pos\", \"neg\"]:\n",
    "            folder_path = os.path.join(\"aclImdb\", phase, label)\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                with open(os.path.join(folder_path, file_name), \"r\") as f:\n",
    "                    comment = f.read().lower()\n",
    "                    tokenized_comment = self.tokenizer(comment, padding=\"max_length\", truncation=True, max_length=embedding_size)\n",
    "                    self.comments.append(tokenized_comment[\"input_ids\"])\n",
    "                    self.attention_masks.append(tokenized_comment[\"attention_mask\"])\n",
    "                    if label == \"neg\":\n",
    "                        self.labels.append([0])\n",
    "                    else:\n",
    "                        self.labels.append([1])\n",
    "                        \n",
    "        assert len(self.comments) == len(self.labels) == len(self.attention_masks)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.comments[index]), torch.LongTensor(self.attention_masks[index]), torch.LongTensor(self.labels[index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement d'un model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces hyperparamètres sont arbitraires.\n",
    "\n",
    "Sauf vocab_size, qui est le nombre de tokens dans le BertTokenizer de HuggingFace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 500\n",
    "num_epochs = 10\n",
    "vocab_size = 30522\n",
    "embedding_size = 200\n",
    "num_classes = 2\n",
    "num_layers = 2\n",
    "heads = 8\n",
    "forward_expansion = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 donnees d'entrainement\n",
      "25000 donnees de test\n",
      "torch.Size([500, 200])\n",
      "torch.Size([500, 200])\n",
      "torch.Size([500, 1])\n"
     ]
    }
   ],
   "source": [
    "# Define data loaders\n",
    "train_dataset = ImdbDataset(\"train\", embedding_size)\n",
    "test_dataset = ImdbDataset(\"test\", embedding_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "print(f\"{len(train_dataset)} donnees d'entrainement\")\n",
    "print(f\"{len(test_dataset)} donnees de test\")\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "print(data[0].shape)\n",
    "print(data[1].shape)\n",
    "print(data[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1777, -0.1561],\n",
      "        [ 0.2121, -0.2174],\n",
      "        [ 0.1872, -0.1525],\n",
      "        [ 0.2167, -0.2095],\n",
      "        [ 0.1825, -0.1686],\n",
      "        [ 0.2439, -0.1700],\n",
      "        [ 0.2166, -0.2331],\n",
      "        [ 0.2802, -0.1709],\n",
      "        [ 0.2227, -0.2128],\n",
      "        [ 0.1927, -0.1581],\n",
      "        [ 0.2472, -0.2224],\n",
      "        [ 0.1911, -0.1556],\n",
      "        [ 0.3271, -0.2087],\n",
      "        [ 0.1938, -0.2079],\n",
      "        [ 0.2350, -0.1938],\n",
      "        [ 0.2579, -0.2449],\n",
      "        [ 0.1463, -0.2378],\n",
      "        [ 0.2679, -0.2249],\n",
      "        [ 0.1996, -0.1564],\n",
      "        [ 0.2239, -0.2625],\n",
      "        [ 0.2328, -0.1913],\n",
      "        [ 0.2347, -0.1004],\n",
      "        [ 0.1794, -0.1617],\n",
      "        [ 0.2817, -0.2058],\n",
      "        [ 0.1961, -0.1419],\n",
      "        [ 0.1938, -0.2175],\n",
      "        [ 0.2726, -0.2031],\n",
      "        [ 0.2253, -0.1753],\n",
      "        [ 0.2078, -0.2520],\n",
      "        [ 0.2527, -0.1760],\n",
      "        [ 0.2486, -0.1848],\n",
      "        [ 0.2186, -0.0845],\n",
      "        [ 0.2229, -0.1824],\n",
      "        [ 0.2435, -0.1611],\n",
      "        [ 0.2106, -0.2147],\n",
      "        [ 0.2114, -0.1883],\n",
      "        [ 0.1235, -0.1965],\n",
      "        [ 0.2220, -0.1635],\n",
      "        [ 0.2012, -0.1817],\n",
      "        [ 0.2318, -0.2060],\n",
      "        [ 0.2724, -0.2084],\n",
      "        [ 0.1739, -0.2328],\n",
      "        [ 0.2065, -0.1642],\n",
      "        [ 0.2198, -0.1450],\n",
      "        [ 0.1958, -0.2508],\n",
      "        [ 0.2818, -0.2354],\n",
      "        [ 0.2007, -0.1358],\n",
      "        [ 0.2104, -0.2781],\n",
      "        [ 0.1862, -0.1414],\n",
      "        [ 0.2066, -0.1561],\n",
      "        [ 0.2910, -0.1896],\n",
      "        [ 0.1394, -0.1801],\n",
      "        [ 0.2110, -0.2311],\n",
      "        [ 0.2191, -0.2234],\n",
      "        [ 0.2430, -0.1752],\n",
      "        [ 0.1857, -0.1479],\n",
      "        [ 0.2099, -0.1379],\n",
      "        [ 0.1619, -0.1938],\n",
      "        [ 0.2356, -0.2252],\n",
      "        [ 0.1877, -0.2558],\n",
      "        [ 0.2978, -0.1817],\n",
      "        [ 0.1534, -0.1615],\n",
      "        [ 0.2220, -0.1792],\n",
      "        [ 0.1804, -0.1469],\n",
      "        [ 0.1613, -0.1309],\n",
      "        [ 0.1079, -0.1740],\n",
      "        [ 0.2822, -0.1739],\n",
      "        [ 0.2856, -0.2426],\n",
      "        [ 0.1614, -0.2426],\n",
      "        [ 0.1879, -0.1776],\n",
      "        [ 0.2589, -0.1630],\n",
      "        [ 0.1963, -0.1939],\n",
      "        [ 0.1635, -0.2098],\n",
      "        [ 0.2653, -0.2163],\n",
      "        [ 0.2622, -0.2023],\n",
      "        [ 0.2159, -0.2121],\n",
      "        [ 0.2748, -0.2133],\n",
      "        [ 0.2764, -0.1992],\n",
      "        [ 0.1731, -0.2220],\n",
      "        [ 0.2869, -0.2032],\n",
      "        [ 0.1246, -0.1815],\n",
      "        [ 0.1207, -0.2406],\n",
      "        [ 0.2004, -0.1359],\n",
      "        [ 0.2330, -0.1405],\n",
      "        [ 0.1965, -0.1607],\n",
      "        [ 0.2509, -0.2315],\n",
      "        [ 0.2112, -0.1561],\n",
      "        [ 0.1913, -0.1695],\n",
      "        [ 0.2126, -0.2119],\n",
      "        [ 0.2504, -0.1900],\n",
      "        [ 0.2909, -0.1946],\n",
      "        [ 0.1563, -0.1196],\n",
      "        [ 0.1895, -0.1643],\n",
      "        [ 0.2030, -0.1227],\n",
      "        [ 0.1762, -0.1534],\n",
      "        [ 0.2171, -0.2332],\n",
      "        [ 0.1993, -0.1450],\n",
      "        [ 0.2039, -0.2307],\n",
      "        [ 0.2128, -0.2454],\n",
      "        [ 0.2470, -0.2009],\n",
      "        [ 0.1682, -0.1686],\n",
      "        [ 0.2522, -0.2113],\n",
      "        [ 0.1404, -0.1575],\n",
      "        [ 0.1564, -0.1296],\n",
      "        [ 0.1940, -0.0743],\n",
      "        [ 0.2016, -0.1833],\n",
      "        [ 0.2130, -0.1811],\n",
      "        [ 0.2312, -0.1943],\n",
      "        [ 0.2195, -0.1961],\n",
      "        [ 0.2765, -0.1786],\n",
      "        [ 0.1808, -0.1402],\n",
      "        [ 0.2750, -0.1840],\n",
      "        [ 0.2146, -0.2360],\n",
      "        [ 0.2453, -0.1225],\n",
      "        [ 0.1864, -0.1454],\n",
      "        [ 0.2469, -0.1803],\n",
      "        [ 0.1814, -0.1798],\n",
      "        [ 0.2022, -0.1797],\n",
      "        [ 0.1154, -0.1985],\n",
      "        [ 0.2432, -0.1742],\n",
      "        [ 0.1615, -0.0341],\n",
      "        [ 0.2656, -0.1809],\n",
      "        [ 0.3269, -0.2545],\n",
      "        [ 0.2856, -0.2186],\n",
      "        [ 0.2287, -0.2169],\n",
      "        [ 0.2647, -0.2139],\n",
      "        [ 0.2319, -0.1493],\n",
      "        [ 0.2125, -0.2533],\n",
      "        [ 0.2380, -0.1749],\n",
      "        [ 0.2459, -0.1273],\n",
      "        [ 0.2238, -0.1710],\n",
      "        [ 0.1590, -0.2137],\n",
      "        [ 0.2422, -0.1785],\n",
      "        [ 0.2770, -0.1444],\n",
      "        [ 0.2510, -0.1741],\n",
      "        [ 0.2150, -0.1843],\n",
      "        [ 0.3444, -0.1957],\n",
      "        [ 0.1904, -0.1517],\n",
      "        [ 0.2887, -0.1158],\n",
      "        [ 0.2323, -0.2913],\n",
      "        [ 0.2699, -0.1041],\n",
      "        [ 0.2349, -0.1259],\n",
      "        [ 0.2244, -0.1315],\n",
      "        [ 0.2238, -0.1667],\n",
      "        [ 0.1925, -0.1732],\n",
      "        [ 0.1153, -0.1745],\n",
      "        [ 0.1640, -0.2397],\n",
      "        [ 0.2392, -0.3043],\n",
      "        [ 0.2728, -0.1780],\n",
      "        [ 0.2320, -0.1507],\n",
      "        [ 0.2462, -0.1612],\n",
      "        [ 0.2348, -0.1662],\n",
      "        [ 0.2062, -0.2230],\n",
      "        [ 0.2069, -0.1229],\n",
      "        [ 0.1740, -0.1203],\n",
      "        [ 0.2111, -0.1899],\n",
      "        [ 0.2262, -0.1963],\n",
      "        [ 0.1913, -0.1869],\n",
      "        [ 0.1689, -0.1502],\n",
      "        [ 0.1987, -0.1780],\n",
      "        [ 0.2320, -0.1772],\n",
      "        [ 0.2210, -0.2031],\n",
      "        [ 0.2308, -0.2295],\n",
      "        [ 0.1812, -0.1791],\n",
      "        [ 0.1957, -0.2132],\n",
      "        [ 0.1655, -0.1721],\n",
      "        [ 0.2936, -0.1893],\n",
      "        [ 0.2498, -0.2175],\n",
      "        [ 0.2500, -0.1726],\n",
      "        [ 0.2368, -0.1886],\n",
      "        [ 0.2088, -0.1460],\n",
      "        [ 0.2293, -0.1750],\n",
      "        [ 0.2173, -0.2045],\n",
      "        [ 0.1733, -0.1639],\n",
      "        [ 0.1232, -0.2572],\n",
      "        [ 0.1586, -0.1569],\n",
      "        [ 0.1976, -0.1831],\n",
      "        [ 0.2817, -0.1851],\n",
      "        [ 0.3157, -0.2694],\n",
      "        [ 0.2552, -0.1949],\n",
      "        [ 0.1701, -0.1906],\n",
      "        [ 0.2075, -0.2196],\n",
      "        [ 0.2813, -0.2790],\n",
      "        [ 0.1866, -0.2327],\n",
      "        [ 0.2173, -0.2956],\n",
      "        [ 0.2212, -0.1674],\n",
      "        [ 0.2075, -0.1574],\n",
      "        [ 0.1900, -0.2283],\n",
      "        [ 0.2715, -0.2268],\n",
      "        [ 0.2319, -0.1699],\n",
      "        [ 0.2205, -0.2379],\n",
      "        [ 0.2213, -0.1674],\n",
      "        [ 0.1411, -0.1891],\n",
      "        [ 0.2342, -0.2058],\n",
      "        [ 0.2456, -0.2637],\n",
      "        [ 0.2510, -0.2480],\n",
      "        [ 0.2195, -0.1725],\n",
      "        [ 0.2274, -0.1196],\n",
      "        [ 0.2163, -0.1525],\n",
      "        [ 0.1684, -0.0814],\n",
      "        [ 0.1546, -0.2032],\n",
      "        [ 0.2134, -0.1325],\n",
      "        [ 0.3496, -0.1634],\n",
      "        [ 0.2437, -0.2156],\n",
      "        [ 0.1980, -0.1942],\n",
      "        [ 0.1619, -0.1591],\n",
      "        [ 0.1953, -0.1259],\n",
      "        [ 0.2750, -0.1492],\n",
      "        [ 0.1536, -0.2176],\n",
      "        [ 0.2951, -0.1928],\n",
      "        [ 0.2678, -0.2319],\n",
      "        [ 0.2493, -0.1792],\n",
      "        [ 0.1547, -0.1815],\n",
      "        [ 0.2605, -0.1679],\n",
      "        [ 0.2617, -0.1872],\n",
      "        [ 0.1387, -0.2095],\n",
      "        [ 0.1655, -0.2402],\n",
      "        [ 0.1870, -0.1106],\n",
      "        [ 0.2211, -0.2231],\n",
      "        [ 0.2040, -0.2707],\n",
      "        [ 0.2636, -0.2327],\n",
      "        [ 0.1709, -0.1576],\n",
      "        [ 0.2290, -0.1952],\n",
      "        [ 0.2672, -0.2447],\n",
      "        [ 0.2639, -0.1323],\n",
      "        [ 0.2383, -0.1256],\n",
      "        [ 0.2671, -0.2477],\n",
      "        [ 0.1592, -0.2128],\n",
      "        [ 0.1736, -0.2409],\n",
      "        [ 0.2123, -0.1487],\n",
      "        [ 0.1601, -0.2218],\n",
      "        [ 0.2190, -0.2340],\n",
      "        [ 0.1939, -0.1548],\n",
      "        [ 0.2027, -0.1020],\n",
      "        [ 0.2423, -0.1710],\n",
      "        [ 0.2468, -0.2405],\n",
      "        [ 0.2716, -0.1895],\n",
      "        [ 0.2411, -0.2426],\n",
      "        [ 0.2463, -0.2695],\n",
      "        [ 0.2188, -0.1814],\n",
      "        [ 0.2024, -0.2058],\n",
      "        [ 0.2448, -0.2016],\n",
      "        [ 0.1694, -0.2091],\n",
      "        [ 0.1619, -0.1698],\n",
      "        [ 0.2252, -0.1823],\n",
      "        [ 0.2400, -0.1671],\n",
      "        [ 0.2614, -0.1977],\n",
      "        [ 0.2086, -0.2299],\n",
      "        [ 0.1885, -0.1685],\n",
      "        [ 0.1872, -0.1993],\n",
      "        [ 0.2796, -0.1687],\n",
      "        [ 0.2427, -0.2037],\n",
      "        [ 0.2158, -0.2320],\n",
      "        [ 0.2707, -0.2248],\n",
      "        [ 0.1354, -0.1734],\n",
      "        [ 0.1723, -0.2475],\n",
      "        [ 0.2317, -0.1709],\n",
      "        [ 0.1899, -0.2072],\n",
      "        [ 0.1940, -0.2116],\n",
      "        [ 0.1917, -0.2903],\n",
      "        [ 0.1671, -0.1654],\n",
      "        [ 0.2017, -0.2278],\n",
      "        [ 0.1752, -0.1187],\n",
      "        [ 0.1443, -0.1364],\n",
      "        [ 0.2566, -0.2758],\n",
      "        [ 0.2037, -0.2431],\n",
      "        [ 0.2202, -0.2286],\n",
      "        [ 0.2509, -0.1725],\n",
      "        [ 0.2006, -0.2674],\n",
      "        [ 0.2358, -0.1320],\n",
      "        [ 0.2336, -0.1485],\n",
      "        [ 0.2435, -0.2189],\n",
      "        [ 0.1363, -0.1547],\n",
      "        [ 0.2235, -0.1917],\n",
      "        [ 0.2310, -0.1725],\n",
      "        [ 0.1872, -0.2000],\n",
      "        [ 0.2665, -0.2537],\n",
      "        [ 0.1723, -0.1536],\n",
      "        [ 0.2134, -0.1965],\n",
      "        [ 0.2273, -0.1021],\n",
      "        [ 0.1930, -0.2519],\n",
      "        [ 0.2303, -0.1403],\n",
      "        [ 0.2044, -0.1692],\n",
      "        [ 0.2135, -0.2717],\n",
      "        [ 0.2872, -0.1646],\n",
      "        [ 0.1662, -0.1897],\n",
      "        [ 0.2239, -0.1340],\n",
      "        [ 0.2836, -0.2022],\n",
      "        [ 0.1765, -0.1564],\n",
      "        [ 0.2317, -0.1460],\n",
      "        [ 0.2525, -0.2893],\n",
      "        [ 0.2349, -0.1925],\n",
      "        [ 0.2360, -0.1810],\n",
      "        [ 0.1618, -0.2170],\n",
      "        [ 0.2510, -0.2082],\n",
      "        [ 0.1565, -0.2265],\n",
      "        [ 0.1684, -0.1923],\n",
      "        [ 0.2859, -0.2654],\n",
      "        [ 0.1604, -0.2075],\n",
      "        [ 0.2478, -0.2644],\n",
      "        [ 0.2147, -0.1548],\n",
      "        [ 0.1930, -0.1984],\n",
      "        [ 0.2302, -0.0686],\n",
      "        [ 0.1858, -0.2613],\n",
      "        [ 0.1294, -0.2438],\n",
      "        [ 0.1930, -0.2112],\n",
      "        [ 0.2693, -0.1576],\n",
      "        [ 0.2408, -0.2090],\n",
      "        [ 0.2623, -0.2176],\n",
      "        [ 0.2645, -0.1973],\n",
      "        [ 0.1964, -0.1770],\n",
      "        [ 0.2082, -0.1543],\n",
      "        [ 0.2531, -0.2397],\n",
      "        [ 0.1731, -0.2183],\n",
      "        [ 0.2663, -0.1552],\n",
      "        [ 0.2366, -0.1948],\n",
      "        [ 0.2397, -0.2098],\n",
      "        [ 0.1629, -0.2822],\n",
      "        [ 0.2373, -0.1783],\n",
      "        [ 0.2649, -0.1219],\n",
      "        [ 0.2205, -0.1551],\n",
      "        [ 0.1825, -0.1393],\n",
      "        [ 0.1408, -0.2321],\n",
      "        [ 0.1482, -0.2654],\n",
      "        [ 0.3322, -0.1844],\n",
      "        [ 0.2608, -0.2269],\n",
      "        [ 0.2435, -0.1615],\n",
      "        [ 0.2224, -0.1813],\n",
      "        [ 0.2614, -0.2034],\n",
      "        [ 0.1995, -0.1578],\n",
      "        [ 0.2224, -0.1823],\n",
      "        [ 0.2558, -0.1770],\n",
      "        [ 0.1532, -0.1909],\n",
      "        [ 0.1697, -0.1362],\n",
      "        [ 0.2125, -0.1907],\n",
      "        [ 0.1967, -0.1927],\n",
      "        [ 0.2394, -0.2207],\n",
      "        [ 0.2209, -0.2073],\n",
      "        [ 0.2808, -0.2203],\n",
      "        [ 0.2630, -0.1653],\n",
      "        [ 0.2343, -0.2139],\n",
      "        [ 0.1626, -0.0981],\n",
      "        [ 0.2366, -0.2154],\n",
      "        [ 0.2562, -0.1901],\n",
      "        [ 0.1483, -0.1433],\n",
      "        [ 0.2453, -0.1176],\n",
      "        [ 0.1805, -0.1500],\n",
      "        [ 0.2127, -0.1722],\n",
      "        [ 0.1667, -0.2363],\n",
      "        [ 0.1868, -0.1536],\n",
      "        [ 0.2089, -0.1901],\n",
      "        [ 0.1230, -0.2394],\n",
      "        [ 0.2450, -0.2231],\n",
      "        [ 0.1820, -0.1541],\n",
      "        [ 0.2184, -0.2281],\n",
      "        [ 0.2678, -0.1586],\n",
      "        [ 0.1848, -0.2840],\n",
      "        [ 0.1488, -0.1696],\n",
      "        [ 0.2185, -0.2472],\n",
      "        [ 0.1446, -0.1736],\n",
      "        [ 0.2873, -0.2170],\n",
      "        [ 0.1708, -0.2455],\n",
      "        [ 0.2493, -0.1325],\n",
      "        [ 0.1980, -0.2206],\n",
      "        [ 0.1901, -0.2054],\n",
      "        [ 0.2425, -0.2170],\n",
      "        [ 0.2801, -0.2475],\n",
      "        [ 0.2275, -0.1831],\n",
      "        [ 0.2125, -0.1910],\n",
      "        [ 0.2337, -0.1608],\n",
      "        [ 0.2096, -0.1896],\n",
      "        [ 0.1620, -0.2184],\n",
      "        [ 0.2478, -0.2289],\n",
      "        [ 0.2020, -0.1534],\n",
      "        [ 0.2280, -0.2705],\n",
      "        [ 0.1931, -0.2018],\n",
      "        [ 0.1903, -0.2351],\n",
      "        [ 0.1656, -0.1214],\n",
      "        [ 0.2837, -0.2258],\n",
      "        [ 0.2470, -0.1735],\n",
      "        [ 0.1818, -0.2637],\n",
      "        [ 0.1525, -0.1680],\n",
      "        [ 0.3215, -0.1988],\n",
      "        [ 0.2532, -0.1775],\n",
      "        [ 0.2232, -0.1333],\n",
      "        [ 0.2673, -0.1860],\n",
      "        [ 0.2623, -0.1328],\n",
      "        [ 0.1961, -0.1715],\n",
      "        [ 0.2161, -0.2412],\n",
      "        [ 0.2521, -0.1643],\n",
      "        [ 0.2111, -0.1654],\n",
      "        [ 0.2090, -0.1862],\n",
      "        [ 0.1785, -0.2359],\n",
      "        [ 0.1431, -0.2348],\n",
      "        [ 0.1903, -0.1555],\n",
      "        [ 0.2459, -0.1934],\n",
      "        [ 0.2016, -0.1573],\n",
      "        [ 0.2490, -0.1932],\n",
      "        [ 0.1912, -0.1421],\n",
      "        [ 0.2373, -0.1609],\n",
      "        [ 0.2334, -0.1146],\n",
      "        [ 0.2195, -0.1603],\n",
      "        [ 0.1360, -0.1558],\n",
      "        [ 0.1864, -0.0992],\n",
      "        [ 0.2333, -0.1122],\n",
      "        [ 0.2903, -0.1677],\n",
      "        [ 0.1837, -0.0870],\n",
      "        [ 0.2203, -0.2669],\n",
      "        [ 0.2039, -0.2217],\n",
      "        [ 0.2522, -0.1623],\n",
      "        [ 0.1809, -0.2452],\n",
      "        [ 0.2908, -0.1612],\n",
      "        [ 0.1569, -0.1658],\n",
      "        [ 0.1601, -0.1844],\n",
      "        [ 0.2196, -0.1447],\n",
      "        [ 0.2125, -0.2067],\n",
      "        [ 0.1663, -0.2023],\n",
      "        [ 0.1955, -0.2802],\n",
      "        [ 0.2458, -0.1913],\n",
      "        [ 0.2392, -0.1827],\n",
      "        [ 0.1748, -0.1377],\n",
      "        [ 0.2349, -0.2265],\n",
      "        [ 0.2062, -0.2023],\n",
      "        [ 0.2770, -0.2205],\n",
      "        [ 0.1704, -0.1517],\n",
      "        [ 0.2130, -0.1880],\n",
      "        [ 0.2305, -0.1620],\n",
      "        [ 0.1644, -0.2199],\n",
      "        [ 0.1540, -0.1920],\n",
      "        [ 0.2370, -0.2251],\n",
      "        [ 0.2236, -0.2806],\n",
      "        [ 0.2649, -0.1260],\n",
      "        [ 0.2357, -0.2224],\n",
      "        [ 0.1611, -0.1431],\n",
      "        [ 0.2339, -0.1038],\n",
      "        [ 0.1914, -0.1509],\n",
      "        [ 0.2102, -0.2250],\n",
      "        [ 0.2683, -0.1916],\n",
      "        [ 0.1793, -0.2433],\n",
      "        [ 0.1787, -0.1672],\n",
      "        [ 0.1656, -0.2516],\n",
      "        [ 0.2728, -0.1618],\n",
      "        [ 0.1693, -0.1732],\n",
      "        [ 0.2641, -0.3016],\n",
      "        [ 0.2497, -0.0706],\n",
      "        [ 0.2163, -0.1682],\n",
      "        [ 0.1370, -0.1979],\n",
      "        [ 0.1836, -0.1481],\n",
      "        [ 0.2237, -0.1785],\n",
      "        [ 0.1627, -0.2273],\n",
      "        [ 0.2009, -0.1982],\n",
      "        [ 0.2213, -0.1732],\n",
      "        [ 0.2206, -0.1204],\n",
      "        [ 0.2347, -0.2182],\n",
      "        [ 0.1105, -0.1981],\n",
      "        [ 0.3174, -0.2231],\n",
      "        [ 0.2483, -0.2380],\n",
      "        [ 0.2426, -0.1363],\n",
      "        [ 0.1915, -0.2029],\n",
      "        [ 0.2456, -0.1894],\n",
      "        [ 0.2211, -0.2106],\n",
      "        [ 0.1658, -0.1990],\n",
      "        [ 0.2811, -0.2138],\n",
      "        [ 0.1738, -0.2204],\n",
      "        [ 0.2145, -0.2451],\n",
      "        [ 0.2585, -0.1432],\n",
      "        [ 0.1823, -0.1976],\n",
      "        [ 0.1683, -0.1593],\n",
      "        [ 0.1464, -0.1449],\n",
      "        [ 0.2223, -0.1906],\n",
      "        [ 0.1715, -0.2114],\n",
      "        [ 0.2004, -0.1800],\n",
      "        [ 0.2205, -0.2456],\n",
      "        [ 0.1859, -0.1352],\n",
      "        [ 0.1675, -0.1932],\n",
      "        [ 0.2786, -0.2504],\n",
      "        [ 0.2523, -0.1878],\n",
      "        [ 0.1712, -0.1389],\n",
      "        [ 0.1167, -0.1300],\n",
      "        [ 0.1855, -0.1374],\n",
      "        [ 0.1972, -0.1971],\n",
      "        [ 0.2503, -0.2169],\n",
      "        [ 0.1992, -0.2567],\n",
      "        [ 0.1851, -0.1689],\n",
      "        [ 0.1909, -0.2121],\n",
      "        [ 0.2045, -0.1618],\n",
      "        [ 0.2607, -0.1618],\n",
      "        [ 0.2412, -0.2336],\n",
      "        [ 0.2205, -0.1470],\n",
      "        [ 0.1493, -0.1601],\n",
      "        [ 0.2557, -0.2073],\n",
      "        [ 0.2806, -0.1827],\n",
      "        [ 0.2030, -0.1623],\n",
      "        [ 0.2189, -0.2292],\n",
      "        [ 0.1244, -0.1349],\n",
      "        [ 0.1487, -0.1779],\n",
      "        [ 0.1685, -0.1442],\n",
      "        [ 0.2008, -0.1727],\n",
      "        [ 0.2612, -0.1521],\n",
      "        [ 0.2541, -0.1976]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerClassifier(vocab_size=vocab_size, \n",
    "                 seq_length=embedding_size,\n",
    "                 embedding_size=embedding_size,\n",
    "                 num_classes=num_classes, \n",
    "                 num_layers=num_layers, \n",
    "                 num_heads=heads, \n",
    "                 dropout=dropout,\n",
    "                 forward_expansion=forward_expansion).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "output = model(data[0].to(device))\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:43<00:00,  8.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7371, Train acc: 0.5056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:54<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6930, Test acc: 0.5206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:46<00:00,  8.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6774, Train acc: 0.5741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:47<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6473, Test acc: 0.6212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:36<00:00,  7.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5920, Train acc: 0.6890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:53<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6156, Test acc: 0.6675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:45<00:00,  8.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5421, Train acc: 0.7309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:56<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5768, Test acc: 0.6987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:40<00:00,  8.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5105, Train acc: 0.7524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:54<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5684, Test acc: 0.7098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:42<00:00,  8.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4868, Train acc: 0.7682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:55<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5586, Test acc: 0.7237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:43<00:00,  8.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4650, Train acc: 0.7820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:55<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5487, Test acc: 0.7306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:42<00:00,  8.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4411, Train acc: 0.7958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:56<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5542, Test acc: 0.7328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:43<00:00,  8.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4327, Train acc: 0.8020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:56<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5627, Test acc: 0.7309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:55<00:00,  8.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4075, Train acc: 0.8182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:05<00:00,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5463, Test acc: 0.7418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the training loop\n",
    "for _ in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for x, _, y in tqdm(train_loader):\n",
    "        x, y = x.to(device), y.squeeze(1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred, _ = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc /= len(train_dataset)\n",
    "    print(f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, _, y in tqdm(test_loader):\n",
    "            x, y = x.to(device), y.squeeze(1).to(device)\n",
    "            y_pred, _ = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item() * x.size(0)\n",
    "            test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "    test_loss /= len(test_dataset)\n",
    "    test_acc /= len(test_dataset)\n",
    "    print(f\"Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On atteint environ 75% d'accuracy en test. On est loin de l'état de l'art (96%):\n",
    "https://paperswithcode.com/sota/sentiment-analysis-on-imdb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle pense avec 60.45% de certitude que commentaire:\n",
      "\n",
      "\t This film is terrible. You don't really need to read this review further.If you are planning on watching it, suffice to say - don't (unless you are studying how not to make a good movie).The acting is horrendous... serious amateur hour. Even by the standard of Hollywood action flicks, this is a terrible movie.Don't watch it!!! Go for a jog instead - at least you won't feel like killing yourself.\n",
      "\n",
      " est: négatif.\n",
      "--------------------------------------------------\n",
      "Le modèle pense avec 80.03% de certitude que commentaire:\n",
      "\n",
      "\t This movie was not bad.\n",
      "\n",
      " est: positif.\n",
      "--------------------------------------------------\n",
      "Le modèle pense avec 75.65% de certitude que commentaire:\n",
      "\n",
      "\t This movie was okay. I didn't absolutely love it, but I didn't hate it either.\n",
      "\n",
      " est: positif.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    easy = (\"This film is terrible. You don't really need to read this review further.\"\n",
    "        \"If you are planning on watching it, suffice to say - don't (unless you are studying how not to make a good movie).\"\n",
    "        \"The acting is horrendous... serious amateur hour. Even by the standard of Hollywood action flicks, this is a terrible movie.\"\n",
    "        \"Don't watch it!!! Go for a jog instead - at least you won't feel like killing yourself.\")\n",
    "    easy_tokens = torch.LongTensor(tokenizer(easy, padding=\"max_length\", truncation=True, max_length=embedding_size)[\"input_ids\"]).to(device)\n",
    "    easy_output = torch.softmax(model(easy_tokens.unsqueeze(0)).squeeze(0), dim=0)\n",
    "    easy_decision = \"positif\" if easy_output.argmax().item() == 1 else \"négatif\"\n",
    "    conf_score = easy_output[easy_output.argmax().item()].item() * 100\n",
    "    print(f\"Le modèle pense avec {conf_score:.2f}% de certitude que commentaire:\\n\\n\\t {easy}\\n\\n est: {easy_decision}.\", \"-\"*50, sep=\"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    medium = \"This movie was not bad.\"\n",
    "    medium_tokens = torch.LongTensor(tokenizer(medium, padding=\"max_length\", truncation=True, max_length=embedding_size)[\"input_ids\"]).to(device)\n",
    "    medium_output = torch.softmax(model(medium_tokens.unsqueeze(0)).squeeze(0), dim=0)\n",
    "    medium_decision = \"positif\" if medium_output.argmax().item() == 1 else \"négatif\"\n",
    "    conf_score = medium_output[medium_output.argmax().item()].item() * 100\n",
    "    print(f\"Le modèle pense avec {conf_score:.2f}% de certitude que commentaire:\\n\\n\\t {medium}\\n\\n est: {medium_decision}.\", \"-\"*50, sep=\"\\n\")\n",
    "\n",
    "    hard = \"This movie was okay. I didn't absolutely love it, but I didn't hate it either.\"\n",
    "    hard_tokens = torch.LongTensor(tokenizer(hard, padding=\"max_length\", truncation=True, max_length=embedding_size)[\"input_ids\"]).to(device)\n",
    "    hard_output = torch.softmax(model(hard_tokens.unsqueeze(0)).squeeze(0), dim=0)\n",
    "    hard_decision = \"positif\" if hard_output.argmax().item() == 1 else \"négatif\"\n",
    "    conf_score = hard_output[hard_output.argmax().item()].item() * 100\n",
    "    print(f\"Le modèle pense avec {conf_score:.2f}% de certitude que commentaire:\\n\\n\\t {hard}\\n\\n est: {hard_decision}.\", \"-\"*50, sep=\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
